{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mohd7\\AppData\\Local\\miniconda3\\envs\\llm\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading QTL data...\n",
      "Successfully loaded 11278 papers as JSON array\n",
      "Split data: 9022 training, 2256 validation\n",
      "Loading test data...\n",
      "Successfully loaded 1097 test samples\n",
      "Generating titles with rule-based approach...\n",
      "Evaluating rule-based approach...\n",
      "Training BART model...\n",
      "Training with 9022 examples\n",
      "Max input length: 512, Max target length: 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 2256/2256 [03:44<00:00, 10.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.9111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 2256/2256 [03:44<00:00, 10.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Loss: 0.6501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 2256/2256 [03:44<00:00, 10.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Loss: 0.5456\n",
      "Generating titles with BART...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating titles: 100%|██████████| 1097/1097 [03:25<00:00,  5.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating BART approach...\n",
      "Generating titles with hybrid approach...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating titles: 100%|██████████| 1097/1097 [04:49<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating hybrid approach...\n",
      "\n",
      "=== RESULTS ===\n",
      "\n",
      "RULE-BASED APPROACH:\n",
      "BLEU: 0.0147\n",
      "ROUGE-2: 0.0399\n",
      "ROUGE-L: 0.1770\n",
      "\n",
      "BART APPROACH:\n",
      "BLEU: 0.1293\n",
      "ROUGE-2: 0.2616\n",
      "ROUGE-L: 0.4205\n",
      "\n",
      "HYBRID APPROACH:\n",
      "BLEU: 0.1297\n",
      "ROUGE-2: 0.2616\n",
      "ROUGE-L: 0.4209\n",
      "\n",
      "Best approach based on BLEU score: Hybrid\n",
      "\n",
      "Saved all generated titles to 'generated_titles_comparison.csv'\n",
      "Saved rule_based titles to 'rule_based_titles.csv'\n",
      "Saved bart titles to 'bart_titles.csv'\n",
      "Saved hybrid titles to 'hybrid_titles.csv'\n",
      "\n",
      "Sample predictions from Hybrid approach:\n",
      "\n",
      "Abstract (truncated): Porcine circovirus type 3 (PCV3) is regularly reported in association with various clinical presentations, including porcine dermatitis and nephropath...\n",
      "Ground truth: Detection of porcine circovirus type 3 DNA in serum and semen samples of boars from a German boar stud.\n",
      "Predicted: Detection of porcine circovirus type 3 DNA in boar semen from a German stud supplying semen for artificial insemination.\n",
      "\n",
      "Abstract (truncated): This study investigated using imputed genotypes from non-genotyped animals which  were not in the pedigree for the purpose of genetic selection and im...\n",
      "Ground truth: Imputation of non-genotyped F1 dams to improve genetic gain in swine crossbreeding programs.\n",
      "Predicted: Comparison of imputed genotypes from non-genotyped swine.\n",
      "\n",
      "Abstract (truncated): Castration of male piglets in the United States is conducted without analgesics because no Food and Drug Administration (FDA) approved products are la...\n",
      "Ground truth: Proposed multidimensional pain outcome methodology to demonstrate analgesic drug  efficacy and facilitate future drug approval for piglet castration.\n",
      "Predicted: Validation of a validated multimodal method for quantification of pain in piglets following surgical castration.\n",
      "\n",
      "Abstract (truncated): Alopecia is a condition associated with different etiologies, ranging from hormonal changes to chemotherapy, that affects over 80 million people in th...\n",
      "Ground truth: Nanostructured lipid carriers loaded with an association of minoxidil and latanoprost for targeted topical therapy of alopecia.\n",
      "Predicted: Construction of nanostructured lipid carriers for hair follicle-targeted delivery of minoxidil and latanoprost.\n",
      "\n",
      "Abstract (truncated): BACKGROUND: Acute or chronic irreversible respiratory failure may occur in patients undergoing pneumonectomy. Aim of this study was to determine trans...\n",
      "Ground truth: Genome-wide expression of the residual lung reacting to experimental Pneumonectomy.\n",
      "Predicted: Transcriptome analysis of left pneumonectomy in swine model.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import torch\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "import nltk\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def load_qtl_data(file_path):\n",
    "    \"\"\"\n",
    "    Load QTL data from JSON file with robust error handling\n",
    "    \"\"\"\n",
    "    papers = []\n",
    "    \n",
    "    try:\n",
    "        # First try to load as a JSON array\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, list):\n",
    "                    papers = data\n",
    "                    print(f\"Successfully loaded {len(papers)} papers as JSON array\")\n",
    "                    return papers\n",
    "            except json.JSONDecodeError:\n",
    "                # Not a valid JSON array, continue to line-by-line parsing\n",
    "                pass\n",
    "                \n",
    "        # Try line-by-line JSON objects\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                line = line.strip()\n",
    "                if not line:  # Skip empty lines\n",
    "                    continue\n",
    "                try:\n",
    "                    # Handle case where commas might be missing between objects\n",
    "                    if line.endswith('},') or line.endswith('}'):\n",
    "                        if not line.startswith('{'):\n",
    "                            line = '{' + line\n",
    "                        paper = json.loads(line.rstrip(','))\n",
    "                        papers.append(paper)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error parsing line {i+1}: {e}\")\n",
    "                    # Try to fix common JSON issues\n",
    "                    try:\n",
    "                        if line.endswith(','):\n",
    "                            line = line[:-1]\n",
    "                        if not line.startswith('{'):\n",
    "                            line = '{' + line\n",
    "                        if not line.endswith('}'):\n",
    "                            line = line + '}'\n",
    "                        paper = json.loads(line)\n",
    "                        papers.append(paper)\n",
    "                        print(f\"Fixed and loaded line {i+1}\")\n",
    "                    except:\n",
    "                        print(f\"Skipping problematic line {i+1}\")\n",
    "                        \n",
    "            print(f\"Successfully loaded {len(papers)} papers line by line\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        # Create a small sample dataset for testing\n",
    "        papers = [\n",
    "            {\n",
    "                \"PMID\": \"17179536\", \n",
    "                \"Journal\": \"J Anim Sci. 2007\", \n",
    "                \"Title\": \"Variance component analysis of quantitative trait loci for pork carcass composition\", \n",
    "                \"Abstract\": \"In a previous study, QTL for carcass composition and meat quality were identified...\", \n",
    "                \"Category\": \"1\"\n",
    "            },\n",
    "            {\n",
    "                \"PMID\": \"17177700\", \n",
    "                \"Journal\": \"J Anim Breed Genet\", \n",
    "                \"Title\": \"Single nucleotide polymorphism identification in porcine genes\", \n",
    "                \"Abstract\": \"Pituitary adenylate cyclase-activating polypeptide is a neuropeptide with diverse biological actions...\", \n",
    "                \"Category\": \"0\"\n",
    "            }\n",
    "        ]\n",
    "        print(\"Created a small sample dataset for testing\")\n",
    "    \n",
    "    if len(papers) == 0:\n",
    "        # Create a small sample dataset as fallback\n",
    "        papers = [\n",
    "            {\n",
    "                \"PMID\": \"17179536\", \n",
    "                \"Journal\": \"J Anim Sci. 2007\", \n",
    "                \"Title\": \"Variance component analysis of quantitative trait loci for pork carcass composition\", \n",
    "                \"Abstract\": \"In a previous study, QTL for carcass composition and meat quality were identified...\", \n",
    "                \"Category\": \"1\"\n",
    "            },\n",
    "            {\n",
    "                \"PMID\": \"17177700\", \n",
    "                \"Journal\": \"J Anim Breed Genet\", \n",
    "                \"Title\": \"Single nucleotide polymorphism identification in porcine genes\", \n",
    "                \"Abstract\": \"Pituitary adenylate cyclase-activating polypeptide is a neuropeptide with diverse biological actions...\", \n",
    "                \"Category\": \"0\"\n",
    "            }\n",
    "        ]\n",
    "        print(\"No papers found. Created a small sample dataset.\")\n",
    "    \n",
    "    # Verify data structure and remove PMID\n",
    "    cleaned_papers = []\n",
    "    for paper in papers:\n",
    "        # Skip malformed papers\n",
    "        if not isinstance(paper, dict):\n",
    "            continue\n",
    "            \n",
    "        # Ensure all required fields exist\n",
    "        if not all(key in paper for key in ['Title', 'Abstract']):\n",
    "            continue\n",
    "            \n",
    "        # Create a copy without PMID\n",
    "        paper_copy = {k: v for k, v in paper.items() if k != 'PMID'}\n",
    "        cleaned_papers.append(paper_copy)\n",
    "    \n",
    "    print(f\"Cleaned and prepared {len(cleaned_papers)} valid papers\")\n",
    "    return cleaned_papers\n",
    "\n",
    "def load_test_data(file_path):\n",
    "    \"\"\"\n",
    "    Load test data from TSV file with robust error handling\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try pandas read_csv\n",
    "        df = pd.read_csv(file_path, sep='\\t')\n",
    "        test_data = []\n",
    "        original_titles = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            # Skip rows with missing data\n",
    "            if pd.isna(row['Abstract']) or pd.isna(row['Title']):\n",
    "                continue\n",
    "            \n",
    "            # Include PMID for reference (though we won't use it for prediction)\n",
    "            test_item = {\n",
    "                'Abstract': row['Abstract'],\n",
    "                'Label': row['Label'] if 'Label' in row else 0\n",
    "            }\n",
    "            \n",
    "            # Save PMID separately for reference\n",
    "            if 'PMID' in row and not pd.isna(row['PMID']):\n",
    "                test_item['PMID'] = str(row['PMID'])\n",
    "                \n",
    "            test_data.append(test_item)\n",
    "            original_titles.append(row['Title'])\n",
    "        \n",
    "        print(f\"Successfully loaded {len(test_data)} test samples\")\n",
    "        return test_data, original_titles\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading test data: {e}\")\n",
    "        \n",
    "        # Create sample test data as fallback\n",
    "        test_data = [\n",
    "            {\n",
    "                'Abstract': \"Porcine circovirus type 3 is regularly reported in association with various clinical presentations...\",\n",
    "                'Label': 0\n",
    "            },\n",
    "            {\n",
    "                'Abstract': \"This study investigated using imputed genotypes from non-genotyped animals...\",\n",
    "                'Label': 0\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        original_titles = [\n",
    "            \"Detection of porcine circovirus type 3 DNA in serum and semen samples\",\n",
    "            \"Imputation of non-genotyped F1 dams to improve genetic gain in swine\"\n",
    "        ]\n",
    "        \n",
    "        print(\"Created a small sample test dataset\")\n",
    "        return test_data, original_titles\n",
    "\n",
    "def extract_key_phrases(text):\n",
    "    \"\"\"Extract important phrases from the abstract\"\"\"\n",
    "    # Get scientific terms (capitalized terms)\n",
    "    scientific_terms = re.findall(r'\\b[A-Z][a-zA-Z0-9]+([-\\/][a-zA-Z0-9]+)*\\b', text)\n",
    "    \n",
    "    # Get method terms\n",
    "    method_terms = re.findall(r'\\b(analysis|study|investigation|evaluation|detection|identification|characterization|assessment|method|approach|technique|mapping|imputation)\\b', text.lower())\n",
    "    \n",
    "    # Get species or organisms\n",
    "    species_terms = re.findall(r'\\b(pig|porcine|swine|boar|piglet|animal|livestock|cattle|sheep|chicken|goat)\\b', text.lower())\n",
    "    \n",
    "    # Get important biomedical terms\n",
    "    biomedical_terms = re.findall(r'\\b(gene|protein|genetic|genomic|DNA|RNA|SNP|QTL|marker|chromosome|trait|allele|variant|phenotype|genotype)\\b', text.lower())\n",
    "    \n",
    "    return {\n",
    "        'scientific_terms': list(set(scientific_terms)),\n",
    "        'method_terms': list(set(method_terms)),\n",
    "        'species_terms': list(set(species_terms)),\n",
    "        'biomedical_terms': list(set(biomedical_terms))\n",
    "    }\n",
    "\n",
    "def generate_rule_based_title(abstract):\n",
    "    \"\"\"Generate a title using linguistic patterns common in scientific papers\"\"\"\n",
    "    # Extract key information\n",
    "    key_phrases = extract_key_phrases(abstract)\n",
    "    \n",
    "    # Get first and last sentences\n",
    "    sentences = nltk.sent_tokenize(abstract)\n",
    "    first_sentence = sentences[0] if sentences else \"\"\n",
    "    \n",
    "    # Extract main focus from first sentence\n",
    "    focus_match = re.search(r'(investigat|stud|examin|analyz|assess|determin|evaluat|develop|identif|characteriz)[a-z]* (the|of|how|whether) ([^\\.]+)', first_sentence, re.IGNORECASE)\n",
    "    focus_phrase = focus_match.group(3) if focus_match else \"\"\n",
    "    \n",
    "    # Generate title based on patterns\n",
    "    if key_phrases['scientific_terms'] and key_phrases['method_terms'] and key_phrases['species_terms']:\n",
    "        # Pattern: \"Analysis of [Scientific Term] in [Species]\"\n",
    "        method = key_phrases['method_terms'][0].capitalize()\n",
    "        term = key_phrases['scientific_terms'][0]\n",
    "        species = key_phrases['species_terms'][0]\n",
    "        return f\"{method} of {term} in {species}\"\n",
    "        \n",
    "    elif key_phrases['scientific_terms'] and key_phrases['biomedical_terms']:\n",
    "        # Pattern: \"[Scientific Term] and its effect on [Biomedical Term]\"\n",
    "        term = key_phrases['scientific_terms'][0]\n",
    "        bio_term = key_phrases['biomedical_terms'][0]\n",
    "        return f\"{term} and its association with {bio_term}\"\n",
    "        \n",
    "    elif focus_phrase:\n",
    "        # Extract a concise version of the focus phrase\n",
    "        words = focus_phrase.split()\n",
    "        if len(words) > 8:\n",
    "            focus_phrase = \" \".join(words[:8])\n",
    "        return f\"{focus_phrase.capitalize()}\"\n",
    "        \n",
    "    else:\n",
    "        # Extract key phrases from first sentence\n",
    "        words = first_sentence.split()\n",
    "        if len(words) > 10:\n",
    "            return \" \".join(words[:10])\n",
    "        else:\n",
    "            return first_sentence\n",
    "            \n",
    "def train_bart_model(train_data, val_data=None, model_name=\"facebook/bart-base\", epochs=3):\n",
    "    \"\"\"Fine-tune BART model for title generation\"\"\"\n",
    "    # Load pre-trained model and tokenizer\n",
    "    tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "    \n",
    "    # Check for valid training data\n",
    "    if len(train_data) == 0:\n",
    "        print(\"No training data available. Returning base model.\")\n",
    "        return model, tokenizer\n",
    "        \n",
    "    # Prepare data\n",
    "    train_inputs = []\n",
    "    train_targets = []\n",
    "    \n",
    "    for paper in train_data:\n",
    "        if 'Abstract' in paper and 'Title' in paper:\n",
    "            train_inputs.append(paper['Abstract'])\n",
    "            train_targets.append(paper['Title'])\n",
    "    \n",
    "    if len(train_inputs) == 0:\n",
    "        print(\"No valid training examples found. Returning base model.\")\n",
    "        return model, tokenizer\n",
    "        \n",
    "    print(f\"Training with {len(train_inputs)} examples\")\n",
    "    \n",
    "    # Tokenize inputs and targets\n",
    "    max_input_length = min(512, max([len(tokenizer.encode(text)) for text in train_inputs]))\n",
    "    max_target_length = min(128, max([len(tokenizer.encode(text)) for text in train_targets]))\n",
    "    \n",
    "    print(f\"Max input length: {max_input_length}, Max target length: {max_target_length}\")\n",
    "    \n",
    "    train_encodings = tokenizer(train_inputs, truncation=True, padding='max_length', \n",
    "                               max_length=max_input_length, return_tensors='pt')\n",
    "    target_encodings = tokenizer(train_targets, truncation=True, padding='max_length', \n",
    "                                max_length=max_target_length, return_tensors='pt')\n",
    "    \n",
    "    # Create dataset\n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        train_encodings.input_ids,\n",
    "        train_encodings.attention_mask,\n",
    "        target_encodings.input_ids\n",
    "    )\n",
    "    \n",
    "    # Set batch size based on data size\n",
    "    batch_size = min(4, len(train_dataset))\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Training settings\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "            \n",
    "            # Clear gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_titles(model, tokenizer, test_data, method='bart'):\n",
    "    \"\"\"Generate titles using the specified method\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    generated_titles = []\n",
    "    \n",
    "    for paper in tqdm(test_data, desc=\"Generating titles\"):\n",
    "        abstract = paper['Abstract']\n",
    "        \n",
    "        if method == 'rule':\n",
    "            # Rule-based title generation\n",
    "            title = generate_rule_based_title(abstract)\n",
    "            generated_titles.append(title)\n",
    "            \n",
    "        elif method == 'bart':\n",
    "            # Generate with BART\n",
    "            input_ids = tokenizer(abstract, return_tensors='pt', truncation=True, \n",
    "                                 max_length=512).input_ids.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    max_length=64,\n",
    "                    num_beams=4,\n",
    "                    length_penalty=2.0,\n",
    "                    early_stopping=True,\n",
    "                    no_repeat_ngram_size=2\n",
    "                )\n",
    "            \n",
    "            title = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            generated_titles.append(title)\n",
    "            \n",
    "        elif method == 'hybrid':\n",
    "            # Generate both and select the better one\n",
    "            rule_title = generate_rule_based_title(abstract)\n",
    "            \n",
    "            input_ids = tokenizer(abstract, return_tensors='pt', truncation=True, \n",
    "                                 max_length=512).input_ids.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    max_length=64,\n",
    "                    num_beams=4,\n",
    "                    length_penalty=2.0,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "            \n",
    "            bart_title = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Choose based on quality heuristics\n",
    "            key_phrases = extract_key_phrases(abstract)\n",
    "            bart_quality = sum(1 for term in key_phrases['scientific_terms'] if term.lower() in bart_title.lower())\n",
    "            rule_quality = sum(1 for term in key_phrases['scientific_terms'] if term.lower() in rule_title.lower())\n",
    "            \n",
    "            if len(bart_title.split()) >= 4 and (bart_quality >= rule_quality or len(bart_title) < 100):\n",
    "                generated_titles.append(bart_title)\n",
    "            else:\n",
    "                generated_titles.append(rule_title)\n",
    "    \n",
    "    return generated_titles\n",
    "\n",
    "def evaluate_titles(generated_titles, reference_titles):\n",
    "    \"\"\"Calculate BLEU and ROUGE scores\"\"\"\n",
    "    if len(generated_titles) == 0 or len(reference_titles) == 0:\n",
    "        print(\"No titles to evaluate\")\n",
    "        return {\n",
    "            'bleu': 0.0,\n",
    "            'rouge2': 0.0,\n",
    "            'rougeL': 0.0\n",
    "        }\n",
    "    \n",
    "    # Calculate BLEU scores\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    bleu_scores = []\n",
    "    \n",
    "    for gen, ref in zip(generated_titles, reference_titles):\n",
    "        gen_tokens = gen.lower().split()\n",
    "        ref_tokens = [ref.lower().split()]\n",
    "        \n",
    "        if len(gen_tokens) == 0:\n",
    "            bleu_scores.append(0.0)\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            bleu = sentence_bleu(ref_tokens, gen_tokens, smoothing_function=smoothing)\n",
    "            bleu_scores.append(bleu)\n",
    "        except Exception as e:\n",
    "            print(f\"BLEU calculation error: {e}\")\n",
    "            bleu_scores.append(0.0)\n",
    "    \n",
    "    avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0.0\n",
    "    \n",
    "    # Calculate ROUGE scores\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "    \n",
    "    for gen, ref in zip(generated_titles, reference_titles):\n",
    "        try:\n",
    "            scores = scorer.score(ref, gen)\n",
    "            rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "            rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "        except Exception as e:\n",
    "            print(f\"ROUGE calculation error: {e}\")\n",
    "            rouge2_scores.append(0.0)\n",
    "            rougeL_scores.append(0.0)\n",
    "    \n",
    "    avg_rouge2 = sum(rouge2_scores) / len(rouge2_scores) if rouge2_scores else 0.0\n",
    "    avg_rougeL = sum(rougeL_scores) / len(rougeL_scores) if rougeL_scores else 0.0\n",
    "    \n",
    "    return {\n",
    "        'bleu': avg_bleu,\n",
    "        'rouge2': avg_rouge2,\n",
    "        'rougeL': avg_rougeL\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # File paths - adjust as needed\n",
    "    qtl_json_path = \"QTL_text.json\"\n",
    "    test_tsv_path = \"test_unlabeled.tsv\"\n",
    "    \n",
    "    # Load data with robust error handling\n",
    "    print(\"Loading QTL data...\")\n",
    "    qtl_data = load_qtl_data(qtl_json_path)\n",
    "    \n",
    "    # Split data if we have enough samples\n",
    "    if len(qtl_data) > 5:\n",
    "        train_data, val_data = train_test_split(qtl_data, test_size=0.2, random_state=42)\n",
    "        print(f\"Split data: {len(train_data)} training, {len(val_data)} validation\")\n",
    "    else:\n",
    "        # Use all data for training if small dataset\n",
    "        train_data = qtl_data\n",
    "        val_data = qtl_data\n",
    "        print(f\"Using all {len(qtl_data)} samples for both training and validation\")\n",
    "    \n",
    "    # Load test data\n",
    "    print(\"Loading test data...\")\n",
    "    test_data, original_titles = load_test_data(test_tsv_path)\n",
    "    \n",
    "    # Extract PMIDs from test data for tracking purposes\n",
    "    # (We'll only use these for saving results, not for prediction)\n",
    "    test_pmids = []\n",
    "    for paper in test_data:\n",
    "        if 'PMID' in paper:\n",
    "            test_pmids.append(paper['PMID'])\n",
    "        else:\n",
    "            test_pmids.append('Unknown')\n",
    "    \n",
    "    # Generate titles using rule-based approach\n",
    "    print(\"Generating titles with rule-based approach...\")\n",
    "    rule_based_titles = []\n",
    "    for paper in test_data:\n",
    "        title = generate_rule_based_title(paper['Abstract'])\n",
    "        rule_based_titles.append(title)\n",
    "    \n",
    "    # Evaluate rule-based approach\n",
    "    print(\"Evaluating rule-based approach...\")\n",
    "    rule_metrics = evaluate_titles(rule_based_titles, original_titles)\n",
    "    \n",
    "    # Train BART model with safeguards\n",
    "    print(\"Training BART model...\")\n",
    "    try:\n",
    "        model, tokenizer = train_bart_model(train_data, val_data, epochs=3)\n",
    "        \n",
    "        # Generate titles with BART\n",
    "        print(\"Generating titles with BART...\")\n",
    "        bart_titles = generate_titles(model, tokenizer, test_data, method='bart')\n",
    "        \n",
    "        # Evaluate BART approach\n",
    "        print(\"Evaluating BART approach...\")\n",
    "        bart_metrics = evaluate_titles(bart_titles, original_titles)\n",
    "        \n",
    "        # Generate titles with hybrid approach\n",
    "        print(\"Generating titles with hybrid approach...\")\n",
    "        hybrid_titles = generate_titles(model, tokenizer, test_data, method='hybrid')\n",
    "        \n",
    "        # Evaluate hybrid approach\n",
    "        print(\"Evaluating hybrid approach...\")\n",
    "        hybrid_metrics = evaluate_titles(hybrid_titles, original_titles)\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\n=== RESULTS ===\")\n",
    "        print(\"\\nRULE-BASED APPROACH:\")\n",
    "        print(f\"BLEU: {rule_metrics['bleu']:.4f}\")\n",
    "        print(f\"ROUGE-2: {rule_metrics['rouge2']:.4f}\")\n",
    "        print(f\"ROUGE-L: {rule_metrics['rougeL']:.4f}\")\n",
    "        \n",
    "        print(\"\\nBART APPROACH:\")\n",
    "        print(f\"BLEU: {bart_metrics['bleu']:.4f}\")\n",
    "        print(f\"ROUGE-2: {bart_metrics['rouge2']:.4f}\")\n",
    "        print(f\"ROUGE-L: {bart_metrics['rougeL']:.4f}\")\n",
    "        \n",
    "        print(\"\\nHYBRID APPROACH:\")\n",
    "        print(f\"BLEU: {hybrid_metrics['bleu']:.4f}\")\n",
    "        print(f\"ROUGE-2: {hybrid_metrics['rouge2']:.4f}\")\n",
    "        print(f\"ROUGE-L: {hybrid_metrics['rougeL']:.4f}\")\n",
    "        \n",
    "        # Determine best approach based on BLEU score\n",
    "        approaches = {\n",
    "            'Rule-based': rule_metrics['bleu'],\n",
    "            'BART': bart_metrics['bleu'],\n",
    "            'Hybrid': hybrid_metrics['bleu']\n",
    "        }\n",
    "        best_approach = max(approaches.items(), key=lambda x: x[1])[0]\n",
    "        \n",
    "        print(f\"\\nBest approach based on BLEU score: {best_approach}\")\n",
    "        \n",
    "        # Print sample predictions\n",
    "        best_titles = {\n",
    "            'Rule-based': rule_based_titles,\n",
    "            'BART': bart_titles,\n",
    "            'Hybrid': hybrid_titles\n",
    "        }[best_approach]\n",
    "        \n",
    "        # Save all generated titles to CSV files for comparison\n",
    "        results_df = pd.DataFrame({\n",
    "            'PMID': [paper.get('PMID', 'Unknown') for paper in test_data],\n",
    "            'Original_Title': original_titles,\n",
    "            'Rule_Based_Title': rule_based_titles,\n",
    "            'BART_Title': bart_titles,\n",
    "            'Hybrid_Title': hybrid_titles,\n",
    "            'BLEU_Score': [sentence_bleu([ref.lower().split()], gen.lower().split(), smoothing_function=SmoothingFunction().method1) \n",
    "                           for ref, gen in zip(original_titles, best_titles)]\n",
    "        })\n",
    "        \n",
    "        # Save to CSV\n",
    "        results_df.to_csv('generated_titles_comparison.csv', index=False)\n",
    "        print(f\"\\nSaved all generated titles to 'generated_titles_comparison.csv'\")\n",
    "        \n",
    "        # Also save individual approach results\n",
    "        for approach_name, titles in [('rule_based', rule_based_titles), \n",
    "                                     ('bart', bart_titles), \n",
    "                                     ('hybrid', hybrid_titles)]:\n",
    "            approach_df = pd.DataFrame({\n",
    "                'PMID': [paper.get('PMID', 'Unknown') for paper in test_data],\n",
    "                'Original_Title': original_titles,\n",
    "                'Generated_Title': titles\n",
    "            })\n",
    "            approach_df.to_csv(f'{approach_name}_titles.csv', index=False)\n",
    "            print(f\"Saved {approach_name} titles to '{approach_name}_titles.csv'\")\n",
    "        \n",
    "        print(f\"\\nSample predictions from {best_approach} approach:\")\n",
    "        for i in range(min(5, len(test_data))):\n",
    "            print(f\"\\nAbstract (truncated): {test_data[i]['Abstract'][:150]}...\")\n",
    "            print(f\"Ground truth: {original_titles[i]}\")\n",
    "            print(f\"Predicted: {best_titles[i]}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during model training or evaluation: {e}\")\n",
    "        print(\"Falling back to rule-based approach only\")\n",
    "        \n",
    "        # Print rule-based results\n",
    "        print(\"\\n=== RESULTS ===\")\n",
    "        print(\"\\nRULE-BASED APPROACH:\")\n",
    "        print(f\"BLEU: {rule_metrics['bleu']:.4f}\")\n",
    "        print(f\"ROUGE-2: {rule_metrics['rouge2']:.4f}\")\n",
    "        print(f\"ROUGE-L: {rule_metrics['rougeL']:.4f}\")\n",
    "        \n",
    "        # Print sample predictions\n",
    "        print(\"\\nSample predictions from Rule-based approach:\")\n",
    "        for i in range(min(5, len(test_data))):\n",
    "            print(f\"\\nAbstract (truncated): {test_data[i]['Abstract'][:150]}...\")\n",
    "            print(f\"Ground truth: {original_titles[i]}\")\n",
    "            print(f\"Predicted: {rule_based_titles[i]}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
